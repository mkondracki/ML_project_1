#!/usr/bin/env python
# coding: utf-8

import numpy as np


#build the indices for a desired number of folder
def build_k_indices(y, k_fold, seed):
    """build k indices for k-fold.
    
    Args:
        y:      shape=(N,)
        k_fold: K in K-fold, i.e. the fold num
        seed:   the random seed

    Returns:
        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold

    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)
    array([[3, 2],
           [0, 1]])
    """
    num_row = y.shape[0]
    interval = int(num_row / k_fold)
    np.random.seed(seed)
    indices = np.random.permutation(num_row)
    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]
    return np.array(k_indices)


#polynomial basis functions for input data x, for j=0 up to j=degree.
def build_poly(x, degree):

    
    poly = np.ones((len(x),1))
    for deg in range(1, degree + 1) : 
        poly = np.c_[poly, np.power(x, deg)]

    return poly 

#implement ridge regression.
def ridge_regression(y, tx, lambda_):
   
 
    N, D = tx.shape
    lambda_p = 2*N*lambda_
    
    a = tx.T.dot(tx) + lambda_p*np.identity(D)
    b = tx.T.dot(y)
    w = np.linalg.solve(a,b)
    return w

#return the loss of ridge regression for a fold corresponding to k_indices
def cross_validation(y, x, k_indices, k, lambda_, degree):

    # get k'th subgroup in test, others in train
    ind_te = k_indices[k]
    ind_tr = k_indices[~(np.arange(k_indices.shape[0]) == k)]
    ind_tr = ind_tr.reshape(-1)

    x_te  = x[ind_te]
    x_tr = x[ind_tr]
    y_te  = y[ind_te]
    y_tr = y[ind_tr]

    # form data with polynomial degree
    x_te_p = build_poly(x_te, degree)
    x_tr_p = build_poly(x_tr, degree)

    # ridge regression
    w = ridge_regression(y_tr, x_tr_p , lambda_) 

    # calculate the loss for train and test data
    loss_tr = np.sqrt(2*compute_mse(y_tr,x_tr_p , w)) 
    loss_te = np.sqrt(2*compute_mse(y_te,x_te_p , w))

    return loss_tr, loss_te
